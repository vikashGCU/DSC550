{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Project-7_DSC550.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOV/XDZsukU75tCoBuVcsto",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vikashGCU/DSC550/blob/main/Project_7_DSC550.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5s_MeaVwDJg",
        "outputId": "83d6a1bb-2ada-41a4-e2a0-5d7a6ab7040b"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')\n",
        "\n",
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])\n",
        "\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')\n",
        "\n",
        "\n",
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars\n",
        "\n",
        "ids_from_chars = preprocessing.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)\n",
        "\n",
        "ids = ids_from_chars(chars)\n",
        "ids\n",
        "\n",
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "\n",
        "\n",
        "chars = chars_from_ids(ids)\n",
        "chars\n",
        "\n",
        "tf.strings.reduce_join(chars, axis=-1).numpy()\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "\n",
        "\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
        "\n",
        "\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "\n",
        "\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))\n",
        "\n",
        "\n",
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "\n",
        "\n",
        "split_input_target(list(\"Tensorflow\"))\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())\n",
        "\n",
        "\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset\n",
        "\n",
        "\n",
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "\n",
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "\n",
        "\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "\n",
        "\n",
        "sampled_indices\n",
        "\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "Length of text: 1115394 characters\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "65 unique characters\n",
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n",
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n",
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n",
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n",
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  16896     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  67650     \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Input:\n",
            " b\" as if the tragedy\\nWere play'd in jest by counterfeiting actors?\\nHere on my knee I vow to God above,\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\",aa;Oi-PCWHFLGMCr?!PvBMVZzzg[UNK]bJBVMj\\n'JR rNZYPB3W3mu,dCab-lzY;w&sha:vzUjgWCKbDBbqA!'b-'vyhr$JDcN[UNK]Cd3z\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjX5XrDanLaj"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkDA8ypgnLtq",
        "outputId": "71798581-eff7-4e5d-ddc0-2960e3931c64"
      },
      "source": [
        "#Train the model\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)\n",
        "tf.exp(mean_loss).numpy()\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.190649\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj0lO9yBnXeC"
      },
      "source": [
        "#Configure checkpoints\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL1vhxe1ndlP",
        "outputId": "a4f68a94-0736-4581-8620-53cb7ec6d601"
      },
      "source": [
        "EPOCHS = 500\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "172/172 [==============================] - 11s 50ms/step - loss: 2.7245\n",
            "Epoch 2/500\n",
            "172/172 [==============================] - 9s 50ms/step - loss: 1.9937\n",
            "Epoch 3/500\n",
            "172/172 [==============================] - 10s 51ms/step - loss: 1.7148\n",
            "Epoch 4/500\n",
            "172/172 [==============================] - 10s 51ms/step - loss: 1.5534\n",
            "Epoch 5/500\n",
            "172/172 [==============================] - 10s 52ms/step - loss: 1.4543\n",
            "Epoch 6/500\n",
            "172/172 [==============================] - 10s 52ms/step - loss: 1.3850\n",
            "Epoch 7/500\n",
            "172/172 [==============================] - 10s 53ms/step - loss: 1.3327\n",
            "Epoch 8/500\n",
            "172/172 [==============================] - 10s 53ms/step - loss: 1.2878\n",
            "Epoch 9/500\n",
            "172/172 [==============================] - 10s 53ms/step - loss: 1.2462\n",
            "Epoch 10/500\n",
            "172/172 [==============================] - 10s 53ms/step - loss: 1.2061\n",
            "Epoch 11/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.1671\n",
            "Epoch 12/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.1261\n",
            "Epoch 13/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.0824\n",
            "Epoch 14/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.0368\n",
            "Epoch 15/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.9884\n",
            "Epoch 16/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.9375\n",
            "Epoch 17/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.8854\n",
            "Epoch 18/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.8326\n",
            "Epoch 19/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.7803\n",
            "Epoch 20/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.7324\n",
            "Epoch 21/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.6880\n",
            "Epoch 22/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.6478\n",
            "Epoch 23/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.6125\n",
            "Epoch 24/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.5830\n",
            "Epoch 25/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.5577\n",
            "Epoch 26/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.5357\n",
            "Epoch 27/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.5184\n",
            "Epoch 28/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.5035\n",
            "Epoch 29/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4919\n",
            "Epoch 30/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4802\n",
            "Epoch 31/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4731\n",
            "Epoch 32/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4639\n",
            "Epoch 33/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4580\n",
            "Epoch 34/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.4533\n",
            "Epoch 35/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.4492\n",
            "Epoch 36/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4426\n",
            "Epoch 37/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4363\n",
            "Epoch 38/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4365\n",
            "Epoch 39/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4377\n",
            "Epoch 40/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4362\n",
            "Epoch 41/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4342\n",
            "Epoch 42/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4297\n",
            "Epoch 43/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4339\n",
            "Epoch 44/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4346\n",
            "Epoch 45/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4310\n",
            "Epoch 46/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4318\n",
            "Epoch 47/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4355\n",
            "Epoch 48/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4330\n",
            "Epoch 49/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4278\n",
            "Epoch 50/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4307\n",
            "Epoch 51/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4315\n",
            "Epoch 52/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4354\n",
            "Epoch 53/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.4355\n",
            "Epoch 54/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4429\n",
            "Epoch 55/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.4436\n",
            "Epoch 56/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.4491\n",
            "Epoch 57/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4499\n",
            "Epoch 58/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4503\n",
            "Epoch 59/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4548\n",
            "Epoch 60/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4547\n",
            "Epoch 61/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4533\n",
            "Epoch 62/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4544\n",
            "Epoch 63/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4607\n",
            "Epoch 64/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4635\n",
            "Epoch 65/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4660\n",
            "Epoch 66/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4688\n",
            "Epoch 67/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 0.4676\n",
            "Epoch 68/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4763\n",
            "Epoch 69/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 0.4810\n",
            "Epoch 70/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.4812\n",
            "Epoch 71/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 0.4833\n",
            "Epoch 72/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 0.4954\n",
            "Epoch 73/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 0.5042\n",
            "Epoch 74/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 0.5131\n",
            "Epoch 75/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.5159\n",
            "Epoch 76/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.5183\n",
            "Epoch 77/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.5254\n",
            "Epoch 78/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.5272\n",
            "Epoch 79/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.5340\n",
            "Epoch 80/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.5407\n",
            "Epoch 81/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.5439\n",
            "Epoch 82/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.5491\n",
            "Epoch 83/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.5611\n",
            "Epoch 84/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.5719\n",
            "Epoch 85/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.5728\n",
            "Epoch 86/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.5836\n",
            "Epoch 87/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.5972\n",
            "Epoch 88/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.6103\n",
            "Epoch 89/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.6198\n",
            "Epoch 90/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.6273\n",
            "Epoch 91/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.6270\n",
            "Epoch 92/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.6338\n",
            "Epoch 93/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.6438\n",
            "Epoch 94/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.6614\n",
            "Epoch 95/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.6634\n",
            "Epoch 96/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.6772\n",
            "Epoch 97/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.6805\n",
            "Epoch 98/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.6978\n",
            "Epoch 99/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.7147\n",
            "Epoch 100/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.7380\n",
            "Epoch 101/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.7604\n",
            "Epoch 102/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 0.7720\n",
            "Epoch 103/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.7759\n",
            "Epoch 104/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.7861\n",
            "Epoch 105/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.7938\n",
            "Epoch 106/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.8059\n",
            "Epoch 107/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.8250\n",
            "Epoch 108/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.8486\n",
            "Epoch 109/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.8697\n",
            "Epoch 110/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.8797\n",
            "Epoch 111/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.8979\n",
            "Epoch 112/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.9227\n",
            "Epoch 113/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.9383\n",
            "Epoch 114/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.9541\n",
            "Epoch 115/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.9737\n",
            "Epoch 116/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.0152\n",
            "Epoch 117/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.0185\n",
            "Epoch 118/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.0340\n",
            "Epoch 119/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.0658\n",
            "Epoch 120/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.0898\n",
            "Epoch 121/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.0965\n",
            "Epoch 122/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.0872\n",
            "Epoch 123/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1143\n",
            "Epoch 124/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.1490\n",
            "Epoch 125/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1719\n",
            "Epoch 126/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1935\n",
            "Epoch 127/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1803\n",
            "Epoch 128/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1814\n",
            "Epoch 129/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1959\n",
            "Epoch 130/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1932\n",
            "Epoch 131/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2246\n",
            "Epoch 132/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2402\n",
            "Epoch 133/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2477\n",
            "Epoch 134/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2750\n",
            "Epoch 135/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2962\n",
            "Epoch 136/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2813\n",
            "Epoch 137/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2761\n",
            "Epoch 138/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2856\n",
            "Epoch 139/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.3347\n",
            "Epoch 140/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.3262\n",
            "Epoch 141/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.3120\n",
            "Epoch 142/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.3182\n",
            "Epoch 143/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.3214\n",
            "Epoch 144/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.3161\n",
            "Epoch 145/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.3180\n",
            "Epoch 146/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.3256\n",
            "Epoch 147/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.3555\n",
            "Epoch 148/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.3723\n",
            "Epoch 149/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.3970\n",
            "Epoch 150/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.4169\n",
            "Epoch 151/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.4488\n",
            "Epoch 152/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.4363\n",
            "Epoch 153/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.4149\n",
            "Epoch 154/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.3950\n",
            "Epoch 155/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.3870\n",
            "Epoch 156/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.3893\n",
            "Epoch 157/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.4353\n",
            "Epoch 158/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.4855\n",
            "Epoch 159/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.7612\n",
            "Epoch 160/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 2.0867\n",
            "Epoch 161/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 2.0487\n",
            "Epoch 162/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9630\n",
            "Epoch 163/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9265\n",
            "Epoch 164/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9505\n",
            "Epoch 165/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9826\n",
            "Epoch 166/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 2.0046\n",
            "Epoch 167/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9430\n",
            "Epoch 168/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9200\n",
            "Epoch 169/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9007\n",
            "Epoch 170/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.8843\n",
            "Epoch 171/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.8731\n",
            "Epoch 172/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.8627\n",
            "Epoch 173/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.8569\n",
            "Epoch 174/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.8554\n",
            "Epoch 175/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.8613\n",
            "Epoch 176/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.8750\n",
            "Epoch 177/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.8863\n",
            "Epoch 178/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.8705\n",
            "Epoch 179/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.8728\n",
            "Epoch 180/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.8801\n",
            "Epoch 181/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9060\n",
            "Epoch 182/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.8717\n",
            "Epoch 183/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.8540\n",
            "Epoch 184/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.8460\n",
            "Epoch 185/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.8356\n",
            "Epoch 186/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.8271\n",
            "Epoch 187/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.8195\n",
            "Epoch 188/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.8111\n",
            "Epoch 189/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.8019\n",
            "Epoch 190/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.7929\n",
            "Epoch 191/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.7869\n",
            "Epoch 192/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.7791\n",
            "Epoch 193/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.7673\n",
            "Epoch 194/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.7542\n",
            "Epoch 195/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.7456\n",
            "Epoch 196/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.7335\n",
            "Epoch 197/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.7226\n",
            "Epoch 198/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.7110\n",
            "Epoch 199/500\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.7005\n",
            "Epoch 200/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.6873\n",
            "Epoch 201/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.6726\n",
            "Epoch 202/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.6600\n",
            "Epoch 203/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.6470\n",
            "Epoch 204/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.6350\n",
            "Epoch 205/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.6210\n",
            "Epoch 206/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.6079\n",
            "Epoch 207/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.5961\n",
            "Epoch 208/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.5822\n",
            "Epoch 209/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.5704\n",
            "Epoch 210/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.5582\n",
            "Epoch 211/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.5461\n",
            "Epoch 212/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.5337\n",
            "Epoch 213/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.5198\n",
            "Epoch 214/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.5078\n",
            "Epoch 215/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.4960\n",
            "Epoch 216/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.4832\n",
            "Epoch 217/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.4719\n",
            "Epoch 218/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.4621\n",
            "Epoch 219/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.4485\n",
            "Epoch 220/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.4378\n",
            "Epoch 221/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.4283\n",
            "Epoch 222/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.4179\n",
            "Epoch 223/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.4070\n",
            "Epoch 224/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.3973\n",
            "Epoch 225/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.3880\n",
            "Epoch 226/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.3788\n",
            "Epoch 227/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.3695\n",
            "Epoch 228/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.3610\n",
            "Epoch 229/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.3544\n",
            "Epoch 230/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.3446\n",
            "Epoch 231/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.3365\n",
            "Epoch 232/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.3268\n",
            "Epoch 233/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.3202\n",
            "Epoch 234/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.3132\n",
            "Epoch 235/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.3061\n",
            "Epoch 236/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.3007\n",
            "Epoch 237/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.2941\n",
            "Epoch 238/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.2885\n",
            "Epoch 239/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.2831\n",
            "Epoch 240/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.2748\n",
            "Epoch 241/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2681\n",
            "Epoch 242/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2609\n",
            "Epoch 243/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2526\n",
            "Epoch 244/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2486\n",
            "Epoch 245/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2453\n",
            "Epoch 246/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2379\n",
            "Epoch 247/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2317\n",
            "Epoch 248/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2272\n",
            "Epoch 249/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.2210\n",
            "Epoch 250/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2163\n",
            "Epoch 251/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2085\n",
            "Epoch 252/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2035\n",
            "Epoch 253/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1981\n",
            "Epoch 254/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1945\n",
            "Epoch 255/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1888\n",
            "Epoch 256/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1840\n",
            "Epoch 257/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1817\n",
            "Epoch 258/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1739\n",
            "Epoch 259/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1718\n",
            "Epoch 260/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1650\n",
            "Epoch 261/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1592\n",
            "Epoch 262/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1577\n",
            "Epoch 263/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1536\n",
            "Epoch 264/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.1486\n",
            "Epoch 265/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1446\n",
            "Epoch 266/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1381\n",
            "Epoch 267/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.1363\n",
            "Epoch 268/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1314\n",
            "Epoch 269/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.1297\n",
            "Epoch 270/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1280\n",
            "Epoch 271/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1323\n",
            "Epoch 272/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.1291\n",
            "Epoch 273/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1217\n",
            "Epoch 274/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1211\n",
            "Epoch 275/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1137\n",
            "Epoch 276/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1193\n",
            "Epoch 277/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1110\n",
            "Epoch 278/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1073\n",
            "Epoch 279/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1180\n",
            "Epoch 280/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1143\n",
            "Epoch 281/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1060\n",
            "Epoch 282/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1066\n",
            "Epoch 283/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.0990\n",
            "Epoch 284/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.0986\n",
            "Epoch 285/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1018\n",
            "Epoch 286/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1018\n",
            "Epoch 287/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.0921\n",
            "Epoch 288/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.0915\n",
            "Epoch 289/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.0951\n",
            "Epoch 290/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1142\n",
            "Epoch 291/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1079\n",
            "Epoch 292/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.1037\n",
            "Epoch 293/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.0989\n",
            "Epoch 294/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.0916\n",
            "Epoch 295/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.0928\n",
            "Epoch 296/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.0868\n",
            "Epoch 297/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.0962\n",
            "Epoch 298/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.1039\n",
            "Epoch 299/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.0995\n",
            "Epoch 300/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.0915\n",
            "Epoch 301/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.0880\n",
            "Epoch 302/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.0917\n",
            "Epoch 303/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1087\n",
            "Epoch 304/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.1525\n",
            "Epoch 305/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1401\n",
            "Epoch 306/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1193\n",
            "Epoch 307/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1066\n",
            "Epoch 308/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1083\n",
            "Epoch 309/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1141\n",
            "Epoch 310/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.1216\n",
            "Epoch 311/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.1240\n",
            "Epoch 312/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1231\n",
            "Epoch 313/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.1322\n",
            "Epoch 314/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1337\n",
            "Epoch 315/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1279\n",
            "Epoch 316/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.1604\n",
            "Epoch 317/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.1593\n",
            "Epoch 318/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1449\n",
            "Epoch 319/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1451\n",
            "Epoch 320/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1407\n",
            "Epoch 321/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.1518\n",
            "Epoch 322/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1513\n",
            "Epoch 323/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.1877\n",
            "Epoch 324/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.1737\n",
            "Epoch 325/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.1943\n",
            "Epoch 326/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.2235\n",
            "Epoch 327/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.2759\n",
            "Epoch 328/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.2637\n",
            "Epoch 329/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2723\n",
            "Epoch 330/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.3024\n",
            "Epoch 331/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.3151\n",
            "Epoch 332/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.2968\n",
            "Epoch 333/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.3043\n",
            "Epoch 334/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.2678\n",
            "Epoch 335/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2436\n",
            "Epoch 336/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2562\n",
            "Epoch 337/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2425\n",
            "Epoch 338/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.2585\n",
            "Epoch 339/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2866\n",
            "Epoch 340/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2674\n",
            "Epoch 341/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2603\n",
            "Epoch 342/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.2510\n",
            "Epoch 343/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.2575\n",
            "Epoch 344/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.2691\n",
            "Epoch 345/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.2822\n",
            "Epoch 346/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.2740\n",
            "Epoch 347/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.2862\n",
            "Epoch 348/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.3193\n",
            "Epoch 349/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.3378\n",
            "Epoch 350/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.3899\n",
            "Epoch 351/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.5050\n",
            "Epoch 352/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.3903\n",
            "Epoch 353/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 2.3744\n",
            "Epoch 354/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 2.1681\n",
            "Epoch 355/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0991\n",
            "Epoch 356/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 2.2050\n",
            "Epoch 357/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 2.2824\n",
            "Epoch 358/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 2.1438\n",
            "Epoch 359/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 2.0500\n",
            "Epoch 360/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 2.0052\n",
            "Epoch 361/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9786\n",
            "Epoch 362/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9658\n",
            "Epoch 363/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9554\n",
            "Epoch 364/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9466\n",
            "Epoch 365/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9467\n",
            "Epoch 366/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9583\n",
            "Epoch 367/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9901\n",
            "Epoch 368/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0708\n",
            "Epoch 369/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 2.0639\n",
            "Epoch 370/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 2.0371\n",
            "Epoch 371/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0594\n",
            "Epoch 372/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0380\n",
            "Epoch 373/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0298\n",
            "Epoch 374/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0717\n",
            "Epoch 375/500\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 2.0952\n",
            "Epoch 376/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 2.1064\n",
            "Epoch 377/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0593\n",
            "Epoch 378/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0363\n",
            "Epoch 379/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 2.0141\n",
            "Epoch 380/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 2.0072\n",
            "Epoch 381/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9989\n",
            "Epoch 382/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 2.0005\n",
            "Epoch 383/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9957\n",
            "Epoch 384/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9997\n",
            "Epoch 385/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9895\n",
            "Epoch 386/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9850\n",
            "Epoch 387/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9863\n",
            "Epoch 388/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9928\n",
            "Epoch 389/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9907\n",
            "Epoch 390/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 2.0020\n",
            "Epoch 391/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 2.0208\n",
            "Epoch 392/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0290\n",
            "Epoch 393/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0239\n",
            "Epoch 394/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 2.0182\n",
            "Epoch 395/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 2.0224\n",
            "Epoch 396/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0386\n",
            "Epoch 397/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0422\n",
            "Epoch 398/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 2.0415\n",
            "Epoch 399/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 2.0338\n",
            "Epoch 400/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 2.0290\n",
            "Epoch 401/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0296\n",
            "Epoch 402/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0237\n",
            "Epoch 403/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0265\n",
            "Epoch 404/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0364\n",
            "Epoch 405/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 2.0297\n",
            "Epoch 406/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0223\n",
            "Epoch 407/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0164\n",
            "Epoch 408/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 2.0136\n",
            "Epoch 409/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0138\n",
            "Epoch 410/500\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 2.0119\n",
            "Epoch 411/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 2.0279\n",
            "Epoch 412/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 2.0327\n",
            "Epoch 413/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 2.0320\n",
            "Epoch 414/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 2.0285\n",
            "Epoch 415/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 2.0241\n",
            "Epoch 416/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 2.0165\n",
            "Epoch 417/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0200\n",
            "Epoch 418/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 2.0252\n",
            "Epoch 419/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0201\n",
            "Epoch 420/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 2.0140\n",
            "Epoch 421/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0050\n",
            "Epoch 422/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0043\n",
            "Epoch 423/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9999\n",
            "Epoch 424/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9992\n",
            "Epoch 425/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9966\n",
            "Epoch 426/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9964\n",
            "Epoch 427/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9938\n",
            "Epoch 428/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.9912\n",
            "Epoch 429/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9904\n",
            "Epoch 430/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9908\n",
            "Epoch 431/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9935\n",
            "Epoch 432/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9972\n",
            "Epoch 433/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9945\n",
            "Epoch 434/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9994\n",
            "Epoch 435/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9993\n",
            "Epoch 436/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9970\n",
            "Epoch 437/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0022\n",
            "Epoch 438/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0074\n",
            "Epoch 439/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0049\n",
            "Epoch 440/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 2.0096\n",
            "Epoch 441/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0117\n",
            "Epoch 442/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.0064\n",
            "Epoch 443/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.9988\n",
            "Epoch 444/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9957\n",
            "Epoch 445/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9925\n",
            "Epoch 446/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9932\n",
            "Epoch 447/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.9985\n",
            "Epoch 448/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9944\n",
            "Epoch 449/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.9936\n",
            "Epoch 450/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9969\n",
            "Epoch 451/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.9966\n",
            "Epoch 452/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9957\n",
            "Epoch 453/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9885\n",
            "Epoch 454/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9862\n",
            "Epoch 455/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9867\n",
            "Epoch 456/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9899\n",
            "Epoch 457/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9885\n",
            "Epoch 458/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9877\n",
            "Epoch 459/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9850\n",
            "Epoch 460/500\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.9926\n",
            "Epoch 461/500\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.9868\n",
            "Epoch 462/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9821\n",
            "Epoch 463/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9799\n",
            "Epoch 464/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.9822\n",
            "Epoch 465/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.9836\n",
            "Epoch 466/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.9798\n",
            "Epoch 467/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9793\n",
            "Epoch 468/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9808\n",
            "Epoch 469/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.9817\n",
            "Epoch 470/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9840\n",
            "Epoch 471/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9819\n",
            "Epoch 472/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9868\n",
            "Epoch 473/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9896\n",
            "Epoch 474/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9868\n",
            "Epoch 475/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9856\n",
            "Epoch 476/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9883\n",
            "Epoch 477/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9888\n",
            "Epoch 478/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9866\n",
            "Epoch 479/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9879\n",
            "Epoch 480/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9918\n",
            "Epoch 481/500\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.9888\n",
            "Epoch 482/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9848\n",
            "Epoch 483/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9921\n",
            "Epoch 484/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9922\n",
            "Epoch 485/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9865\n",
            "Epoch 486/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9839\n",
            "Epoch 487/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9848\n",
            "Epoch 488/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9901\n",
            "Epoch 489/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9903\n",
            "Epoch 490/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9887\n",
            "Epoch 491/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9865\n",
            "Epoch 492/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9848\n",
            "Epoch 493/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9841\n",
            "Epoch 494/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9828\n",
            "Epoch 495/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9792\n",
            "Epoch 496/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9764\n",
            "Epoch 497/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9769\n",
            "Epoch 498/500\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9776\n",
            "Epoch 499/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9828\n",
            "Epoch 500/500\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.9932\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "eBUlsMR861Tn",
        "outputId": "4c94705b-30e7-437e-ea39-9736608e1f4e"
      },
      "source": [
        "EPOCHS = 50\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "172/172 [==============================] - 10s 53ms/step - loss: 1.9896\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 10s 53ms/step - loss: 1.9834\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 10s 54ms/step - loss: 1.9851\n",
            "Epoch 4/50\n",
            " 98/172 [================>.............] - ETA: 3s - loss: 1.9827"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-94a677d43a01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUFYy_Zw8oVN"
      },
      "source": [
        "#Text generation\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states\n",
        "\n",
        "\n",
        "\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD3s8M8c88ZE"
      },
      "source": [
        "# Gradient Descent :- The below code use the BPTT (back propagation through time) using the gradient descent method.\n",
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvM6xYYf8-7b"
      },
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mO9XihR9BUu"
      },
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "\n",
        "model.fit(dataset, epochs=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pStJti6J9F9i"
      },
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}